<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#3367D6"/>
  <link rel="apple-touch-icon" href="/icons-192.png">
  <link rel="manifest" href="/manifest.json">
  
  <meta name="generator" content="Hexo 8.1.0">

  
    <meta name="description" content="我永远喜欢成熟可爱的大姐姐">
  

  

  
    <meta name="author" content="寻玉">
  

  

  

  <title>transformer | 寻玉&#39;s blog</title>

  

  
    <link rel="shortcut icon" href="/favicon.ico">
  

  

  

  

  
<link rel="stylesheet" href="/css/style.css">

</head>
<body>
  <div class="root-container">
    
<!-- header container -->
<header class="header-container post">
  
    <div class="post-image" style="background-image: url(http://mms2.baidu.com/it/u=2183424955,1721375183&amp;fm=253&amp;app=138&amp;f=JPEG?w=889&amp;h=500)"></div>
  

  <!-- navbar -->
<nav class="navbar">
  <div class="navbar-content">
    <!-- logo -->
    <div class="navbar-logo">
      <a href="/">
        
          寻玉&#39;s blog
        
      </a>
    </div>
    <!-- link -->
    <div class="navbar-link">
      <div class="navbar-btn">
        <div></div>
        <div></div>
        <div></div>
      </div>
      <ul class="navbar-list">
        
          <li class="navbar-list-item"><a href="/">首页</a></li>
        
          <li class="navbar-list-item"><a href="/archives">归档</a></li>
        
          <li class="navbar-list-item"><a href="/links">友链</a></li>
        
          <li class="navbar-list-item"><a href="/about">关于</a></li>
        
      </ul>
    </div>
  </div>
</nav>

  
  

  
  

  
  

  
  

  
  
    <div class="header-content">
      <div class="post-text layout-block">
        <div class="layout-margin">
          <h1 class="title-wrap">transformer</h1>
          <h2 class="title-sub-wrap">
            <strong>寻玉</strong>
            <span>发布于</span>
            <time  class="article-date" datetime="2025-10-27T00:07:14.980Z" itemprop="datePublished">2025-10-27</time>
          </h2>
          
            <h2 class="last-time">
              <span>最后更新于</span>
              <time  class="article-updated" datetime="2025-10-29T10:17:06.627Z" itemprop="dateUpdated">2025-10-29</time>
            </h2>
          
          
          <ul class="wrap-list dark">
  
</ul>
          <ul class="wrap-list dark">
  
</ul>
        </div>
      </div>
    </div>
  

  
  
  
</header>

    <!-- 文章 -->

<!-- 文章内容 -->
<div class="body-container">
  <article class="content-container layout-block post-container">
    <div class="article-info">
      
      
      
      
      <section class="article-entry markdown-body layout-margin content-padding--large soft-size--large soft-style--box">
        <h1 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h1><p><strong>需要一定的RNN知识，此处不介绍RNN具体知识，相关内容请自己学习</strong></p>
<h2 id="语义表示"><a href="#语义表示" class="headerlink" title="语义表示"></a>语义表示</h2><p>​        在深度学习领域，表示词或者字使用向量方法，即整个语言系统对应一个高维的向量空间，而一个字或者词则对应一个向量，向量间距离的远近则代表了表示词与词之间关系的亲疏（比如<strong>黑人和棉花、鞭子等词的距离就会比较贴近</strong>，而wy和化学的距离就会比较远），这里向量表示词语时包含两个非常重要的问题：<strong>空间的维数应该是多少、向量的长度应该是多少</strong>，也由此引入了几种重要的表示方法：</p>
<ol>
<li>tokenizer（分词器）：整个空间的维数是1，即每个词对应一个唯一的数字，两个词的亲疏关系直接表示为两个数字的绝对值大小，但这种表示无法体现出词语本身复杂的语义而只能表示单个语义（如苹果又可以表示手机又可以表示水果，但苹果只能对应一个数字），且无法表示组合语义（如苹果对应1，香蕉对应2，那么香蕉苹果的组合语义只能对应数字3，但数字3应该对应其他独立词语）</li>
<li>one-hot（独热编码）：整个空间的维数等于词语的个数，而向量的长度为1，也就是说，在这中表示下，所有词对应向量都是相互正交的，这种表示方式一方面需要巨量的空间存储矩阵，另一方面由于向量间距离为0，完全无法表示任何词间关系，但表达组合语义方便。</li>
</ol>
<p><img src="D:\paper\assets\image-20251027094025747.png" alt="image-20251027094025747"></p>
<p>​	上述两种方式体现了极限情况下不同向量空间所能表示的信息密度，<strong>one-hot信息密度过于稀疏而tokenizer的信息密度则过于稠密</strong>，因此现实情况则是<strong>结合这两者</strong>来实现向量空间，即选取一个高维但没那么高维且向量长度不为1的空间。</p>
<h2 id="编解码"><a href="#编解码" class="headerlink" title="编解码"></a>编解码</h2><ul>
<li>seq2seq问题：<strong>序列到序列（Sequence-to-Sequence）<strong>模型（最早是RNN类模型）所解决的一类问题，即</strong>将一个输入序列转换成一个输出序列的任务</strong>。这类问题的核心特点是：<ol>
<li><strong>输入和输出都是序列（sequences）</strong>，例如文本、语音或时间序列（有先后关系）数据。</li>
<li><strong>输入序列和输出序列的长度通常是可变的且不一定相等</strong>。</li>
</ol>
</li>
</ul>
<p>为解决s2s问题的核心特点2（输入长度与输出长度不对应，最初的RNN的输出与输入长度对应），于是提出了编码和解码结构（后来的RNN也有这个编解码的区分）</p>
<h1 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1><h2 id="提出为了解决的问题"><a href="#提出为了解决的问题" class="headerlink" title="提出为了解决的问题"></a>提出为了解决的问题</h2><h3 id="1-难以并行化（限制了训练速度）"><a href="#1-难以并行化（限制了训练速度）" class="headerlink" title="1. 难以并行化（限制了训练速度）"></a>1. 难以并行化（限制了训练速度）</h3><p>这是提出 Transformer 的最主要原因。</p>
<ul>
<li><strong>RNN 的问题：</strong> RNN 在处理序列时是<strong>严格顺序化</strong>的。计算当前时间步（$t$）的隐藏状态和输出时，必须先完成前一个时间步（$t-1$）的计算，因为 $t-1$ 的隐藏状态是 $t$ 的输入之一。这种<strong>顺序依赖性</strong>使得模型无法在训练时充分利用现代 GPU 的并行计算能力。序列越长，训练所需的时间就越久。</li>
<li><strong>Transformer 的解决：</strong> Transformer 完全抛弃了循环和卷积结构，转而完全依赖<strong>自注意力机制（Self-Attention）</strong>。这使得模型可以<strong>同时</strong>处理序列中的所有输入元素，从而实现<strong>高度并行化</strong>，极大地加快了模型的训练速度。</li>
</ul>
<h3 id="2-难以捕捉长距离依赖关系（信息丢失）"><a href="#2-难以捕捉长距离依赖关系（信息丢失）" class="headerlink" title="2. 难以捕捉长距离依赖关系（信息丢失）"></a>2. 难以捕捉长距离依赖关系（信息丢失）</h3><p>虽然 LSTM 和 GRU 缓解了传统 RNN 的梯度消失问题，但在处理<strong>超长序列</strong>时，仍然存在挑战。</p>
<ul>
<li><strong>RNN 的问题：</strong> 序列越长，第一个词的信息需要经过越多的时间步才能传递到最后一个词。在这个长距离的传递过程中，早期信息可能会逐渐<strong>稀释或丢失</strong>（即“长期依赖”问题）。</li>
<li><strong>Transformer 的解决：</strong> 自注意力机制允许模型在计算序列中任一位置（如最后一个词）的表示时，直接<strong>一步到位</strong>地关注到序列中的所有其他位置（如第一个词）。这种“直达”连接使得模型能够更有效地捕捉和利用长距离的依赖关系，而不受序列距离的限制。</li>
</ul>
<h3 id="3-固定长度的上下文向量（Seq2Seq-的瓶颈）"><a href="#3-固定长度的上下文向量（Seq2Seq-的瓶颈）" class="headerlink" title="3. 固定长度的上下文向量（Seq2Seq 的瓶颈）"></a>3. 固定长度的上下文向量（Seq2Seq 的瓶颈）</h3><p>在基于 RNN 的 Seq2Seq 模型中（尤其是在引入注意力机制之前），编码器需要将整个输入序列的信息压缩到一个<strong>固定长度</strong>的上下文向量中。</p>
<ul>
<li><strong>RNN-Seq2Seq 的问题：</strong> 无论输入句子有多长，所有的信息都必须塞进这个固定大小的向量。对于非常长的或信息量大的句子，这个上下文向量很容易成为<strong>信息瓶颈</strong>，导致解码器无法恢复所有必要的信息。</li>
<li><strong>Transformer 的解决：</strong> 虽然不是直接解决，但 Transformer 通过其注意力机制，在解码的每一步都可以<strong>直接访问</strong>编码器输出的<strong>所有</strong>中间状态（即键K和值V）。这相当于提供了一个动态、非固定的“上下文”，消除了固定长度上下文向量带来的信息瓶颈。</li>
</ul>
<h2 id="编解码器总体结构"><a href="#编解码器总体结构" class="headerlink" title="编解码器总体结构"></a>编解码器总体结构</h2><p>结构图如下，图源attention is all you need</p>
<p><img src="D:\paper\assets\image-20251027082059069.png" alt="image-20251027082059069"></p>
<h3 id="inputs"><a href="#inputs" class="headerlink" title="inputs"></a>inputs</h3><p>输入的是一组标签，即每个词都有特殊的编号，并且除了词之外会有类似于意味着开始和提示结束等特殊功能的标签，</p>
<h3 id="input-embedding"><a href="#input-embedding" class="headerlink" title="input embedding"></a>input embedding</h3><p>训练好的input embedding层矩阵E是一个维度为 $V \times d_{model}$ 的矩阵，其中：</p>
<ul>
<li>$V$ 是词汇表的大小（即所有唯一 Token 的数量）。</li>
<li>$d_{model}$ 是你选择的嵌入向量的维度。</li>
</ul>
<p><img src="D:\paper\assets\image-20251027141732750.png" alt="image-20251027141732750"></p>
<p>​	此时，每个token对应E中一行（transformer使用的这种方法查找表做法），按照这种方式，若一句话被tokenizer成n个token，这n个标签会对应E中n个向量，这n个向量又会被整理成一个$n \times d_{model}$ 的矩阵，这就是标签经过input embedding层后获得的结果</p>
<p>​	事实上，由token转化为的嵌入向量的每一个维度本身就有意义，如下图所示，维度可能隐性的表示了这些分类（由于深度学习的不可解释性，这里只是帮助理解，因此是隐性的，并非真实的），这里事实上本质与cnn有一定联系或者异曲同工之妙。</p>
<p><img src="D:\paper\assets\image-20251027142842019.png" alt="image-20251027142842019"></p>
<p>​	前面讲到想要得到token对应的嵌入向量是<strong>需要一个提前训练好的inputs embedding矩阵</strong>，那接下就讲一下如何通过训练获得这个embedding矩阵，主要是下述两种方法：</p>
<ol>
<li>连续词袋模型 (CBOW - Continuous Bag-of-Words)</li>
</ol>
<ul>
<li><strong>工作原理：</strong> CBOW 模型是使用<strong>上下文词语（Context Words）<strong>来</strong>预测</strong>当前的<strong>中心词（Target Word）</strong>。（想象高中物理——力的合成）</li>
<li><strong>输入：</strong> 中心词周围的词语的嵌入向量（上下文）。</li>
<li><strong>输出：</strong> 预测中心词的概率分布。</li>
<li><strong>特点：</strong> 训练速度相对较快，对高频词的处理效果通常更好。</li>
</ul>
<p><img src="D:\paper\assets\image-20251027144444791.png" alt="image-20251027144444791"></p>
<ol start="2">
<li>Skip-gram 模型</li>
</ol>
<ul>
<li><strong>工作原理：</strong> Skip-gram 模型正好相反，它是使用当前的<strong>中心词</strong>来<strong>预测</strong>其周围的<strong>上下文词语</strong>。（想象高中物理——力的分解）</li>
<li><strong>输入：</strong> 中心词的嵌入向量。</li>
<li><strong>输出：</strong> 预测上下文词语的概率分布。</li>
<li><strong>特点：</strong> 训练速度稍慢，但在处理低频词（生僻词）和大型语料库时效果通常更佳。</li>
</ul>
<h3 id="位置编码PE（Postional-encoding）"><a href="#位置编码PE（Postional-encoding）" class="headerlink" title="位置编码PE（Postional encoding）"></a>位置编码PE（Postional encoding）</h3><p>​	位置编码（Positional Encoding）是 Transformer 架构中一个非常巧妙且核心的设计，用于解决其<strong>并行计算带来的顺序信息丢失</strong>问题。</p>
<p><img src="D:\paper\assets\image-20251027143630643.png" alt="image-20251027143630643"></p>
<p>​	最终，位置编码向量 $\mathbf{PE}$ 是通过<strong>向量加法</strong>（Element-wise Addition）的方式，与原始的词嵌入向量 $\mathbf{E}_{\text{word}}$ 结合，形成最终送入编码器和解码器的输入：</p>
<p>​							$$\mathbf{E}<em>{\text{final}} &#x3D; \mathbf{E}</em>{\text{word}} + \mathbf{PE}$$</p>
<p>​	使用周期函数而非简单的学习参数或整数编码，有以下优势：</p>
<ol>
<li><strong>无限序列长度的泛化能力：</strong> 由于是基于函数计算，即使模型在训练中没有见过非常长的句子，它也可以根据函数模式计算出新位置的编码，从而具有<strong>泛化到更长序列的能力</strong>。</li>
<li><strong>相对位置信息的编码：</strong> 正弦和余弦函数的一个关键特性是，对于任意固定的偏移 $k$，$\mathbf{PE}(\text{pos} + k)$ 可以表示为 $\mathbf{PE}(\text{pos})$ 的<strong>线性函数</strong>。这使得模型更容易通过注意力机制来学习和识别<strong>相对位置</strong>（例如，“前一个词”或“相隔两个词”）。</li>
<li><strong>确定性且有界：</strong> 编码值始终在 $[-1, 1]$ 之间，避免了绝对位置索引值过大导致的训练不稳定问题。</li>
</ol>
<h3 id="注意力机制attention"><a href="#注意力机制attention" class="headerlink" title="注意力机制attention"></a>注意力机制attention</h3><p>​	在架构图上写的是multi-attention（多头注意力机制），但这里先不介绍多头注意力机制，先来介绍注意力机制</p>
<p><img src="D:\paper\assets\image-20251027145621205.png" alt="image-20251027145621205"></p>
<p>​	如果你仔细观察了架构图，你会发现在经过PE之后，有三个箭头进入了attention中，这三个箭头意味着attention层中有三个矩阵Q、K、V，这三个矩阵分别的意义是</p>
<h2 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h2><p><img src="D:\paper\assets\image-20251027082324107.png" alt="image-20251027082324107"></p>
<ul>
<li>Decoder-only：代表为GPT，解码器的功能是生成</li>
<li>Encoder-only：代表为BERT，功能是学习和理解文本</li>
<li>Encoder-Decoder：代表为T5、BART，</li>
</ul>
<p><img src="https://github.com/wuyuxp101/my-images/blob/main/icons-192.png?raw=true" alt="科学上网访问"></p>

      </section>

      
      
        <nav class="article-nav">
          
            <div class="article-nav-item layout-padding">
  <article class="card-container article-nav-card content-padding--primary soft-size--large soft-style--box">
    
    <div class="card-text">
      
        <a href="/2025/10/28/hello-world/" itemprop="url">
          <h2 class="card-text--title text-ellipsis">Hello World</h2>
        </a>
      
      <div class="card-text--row">Newer</div>
    </div>
  </article>
</div>
          
          
        </nav>
      

      <section class="page-message-container layout-padding">
        


  
  

  
  


      </section>
    </div>
    <div class="widget-info">
      <section class="widget-author widget-item layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-body">
    
      <img src="http://mms1.baidu.com/it/u=2670436291,1481201037&amp;fm=253&amp;app=138&amp;f=JPEG?w=285&amp;h=285" class="soft-size--round soft-style--box" alt="寻玉">
    
    
      <h2>寻玉</h2>
    
    
      <p>我永远喜欢成熟可爱的大姐姐</p>
    

    <div class="count-box">
      <div class="count-box--item">
        <svg class="icon icon-article" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M240.51564747 647.74217627h196.07203239c16.59071043 0 30.16492806-13.57421762 30.16492805-30.16492806V165.10332731c0-33.18142087-30.16492806-60.32985613-60.32985612-60.32985611H245.04038668C225.43318342 104.7734712 210.35071939 119.85593522 210.35071939 139.46313845V617.57724821c0 16.59071043 13.57421762 30.16492806 30.16492808 30.16492806z m663.62841731-452.47392089v482.63884894c0 33.18142087-27.14843525 60.32985613-60.32985612 60.32985613H180.18579134c-33.18142087 0-60.32985613-27.14843525-60.32985612-60.32985613V195.26825538c-49.77213131 0-90.49478418 40.72265287-90.49478417 90.49478417v452.4739209c0 49.77213131 40.72265287 90.49478418 90.49478417 90.49478417h286.56681657c16.59071043 0 30.16492806 13.57421762 30.16492807 30.16492807s13.57421762 30.16492806 30.16492805 30.16492806h90.49478418c16.59071043 0 30.16492806-13.57421762 30.16492805-30.16492806s13.57421762-30.16492806 30.16492807-30.16492807h286.56681657c49.77213131 0 90.49478418-40.72265287 90.49478417-90.49478417V285.76303955c0-49.77213131-40.72265287-90.49478418-90.49478417-90.49478417zM587.41232014 647.74217627h191.54729318c19.60720323 0 34.68966726-15.08246403 34.68966729-34.68966727V134.93839925c0-16.59071043-13.57421762-30.16492806-30.16492808-30.16492805H617.57724821c-30.16492806 0-60.32985613 27.14843525-60.32985612 60.32985611v452.4739209c0 16.59071043 13.57421762 30.16492806 30.16492805 30.16492806z" fill="currentColor"></path>
</svg>
        <span>2</span>
      </div>
      <div class="count-box--item">
        <svg class="icon icon-categories" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M900.3614811 257.09082106h-339.81629553l-67.96326003-101.9448889c-19.41807444-29.12711113-48.54518557-43.69066667-82.52681443-43.69066667H123.6385189c-53.39970333 0-97.09036999 43.69066667-97.09037113 97.09036999v582.54222222c0 53.39970333 43.69066667 97.09036999 97.09037113 97.09037002h776.7229622c53.39970333 0 97.09036999-43.69066667 97.09037113-97.09037002V354.18119104c0-53.39970333-43.69066667-97.09036999-97.09037113-97.09036998z m-97.09036999 242.72592554H220.72888889c-24.27259221 0-48.54518557-24.27259221-48.54518556-48.54518556s24.27259221-48.54518557 48.54518556-48.54518444h582.54222222c24.27259221 0 48.54518557 24.27259221 48.54518556 48.54518444s-24.27259221 48.54518557-48.54518556 48.54518556z" fill="currentColor"></path>
</svg>
        0
      </div>
      <div class="count-box--item">
        <svg class="icon icon-tags" viewBox="0 0 1098 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M283.42180005 272q0-28.38857157-20.09142843-48.48000001t-48.47999998-20.09142842-48.48000002 20.09142842-20.09142846 48.48000001 20.09142846 48.48 48.48000002 20.09142843 48.47999998-20.09142843 20.09142843-48.48zM855.0332285 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.03999997 263.58857157q-20.9142853 19.81714313-48.75428534 19.81714312-28.38857157 0-48.20571468-19.81714312l-383.04-383.58857157q-20.36571468-19.81714313-34.55999999-54.10285688t-14.19428534-62.6742853l0-222.85714313q0-27.84000002 20.36571469-48.20571469t48.2057147-20.36571466l222.85714313 0q28.38857157 0 62.6742853 14.19428529t54.65142842 34.55999999l383.04000001 382.49142843q19.81714313 20.9142853 19.81714314 48.75428532zM1060.74751475 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.04 263.58857157q-20.9142853 19.81714313-48.75428531 19.81714312-19.26857155 0-31.61142843-7.47428531t-28.38857159-24.13714314l251.79428534-251.7942853q19.81714313-19.81714313 19.81714308-48.20571469 0-27.84000002-19.81714308-48.75428531l-383.04000001-382.49142845q-20.36571468-20.36571468-54.65142842-34.55999999t-62.67428532-14.19428534l120 0q28.38857157 0 62.67428532 14.19428534t54.65142842 34.55999999l383.03999998 382.49142845q19.81714313 20.9142853 19.81714314 48.75428531z" fill="currentColor"></path>
</svg>
        0
      </div>
    </div>
  </div>
</section>

      

      
<section class="widet-notice widget-item layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-title">
    <svg class="icon icon-notice" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M512 945.02305225v28.15620663a24.27259221 24.27259221 0 0 1-24.27259221 24.27259335H394.0352a48.54518557 48.54518557 0 0 1-41.74885888-23.78714112l-110.68302222-184.47170332a132.04290333 132.04290333 0 0 1-17.47626667-48.54518557h118.4502511a200.97706667 200.97706667 0 0 1 76.21594113 14.56355556l20.38897777 133.49925888a48.54518557 48.54518557 0 0 0 36.40888888 27.67075555l16.01991111 2.91271112a24.27259221 24.27259221 0 0 1 20.38897778 25.72894889zM997.45185223 463.45481443a194.18074112 194.18074112 0 0 1-38.8361489 116.50844445 24.75804445 24.75804445 0 0 1-36.4088889 0l-34.95253333-34.95253333a24.27259221 24.27259221 0 0 1-2.91271111-30.58346667 97.09036999 97.09036999 0 0 0 0-106.79940665 24.27259221 24.27259221 0 0 1 2.91271111-30.58346666l34.95253333-34.95253334a24.75804445 24.75804445 0 0 1 18.93262223-7.28177777 26.2144 26.2144 0 0 1 17.47626667 9.70903665A194.18074112 194.18074112 0 0 1 997.45185223 463.45481443z m-194.18074112-388.36148111v776.72296335a48.54518557 48.54518557 0 0 1-48.54518556 48.54518443h-28.64165888a48.54518557 48.54518557 0 0 1-33.98163001-14.07810332l-145.63555556-143.20829668A291.27111111 291.27111111 0 0 0 342.57730333 657.63555556H172.18370333a145.63555556 145.63555556 0 0 1-145.63555556-145.63555556v-97.09036999a145.63555556 145.63555556 0 0 1 145.63555556-145.63555556h170.3936a291.27111111 291.27111111 0 0 0 206.31703779-85.43952668l145.63555555-143.20829554a48.54518557 48.54518557 0 0 1 33.98162888-14.07810446H754.72592555a48.54518557 48.54518557 0 0 1 48.54518556 48.54518555z" fill="currentColor"></path>
</svg>
    <span>NOTICE</span>
  </div>
  <div class="widget-body">
    <p>图床为github仓库,因此需科学上网才能访问 黑色主题视觉舒适,白色主题适合看文本内容</p>
  </div>
</section>


      <section class="widget-categories widget-item layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-title">
    <svg class="icon icon-categories" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M900.3614811 257.09082106h-339.81629553l-67.96326003-101.9448889c-19.41807444-29.12711113-48.54518557-43.69066667-82.52681443-43.69066667H123.6385189c-53.39970333 0-97.09036999 43.69066667-97.09037113 97.09036999v582.54222222c0 53.39970333 43.69066667 97.09036999 97.09037113 97.09037002h776.7229622c53.39970333 0 97.09036999-43.69066667 97.09037113-97.09037002V354.18119104c0-53.39970333-43.69066667-97.09036999-97.09037113-97.09036998z m-97.09036999 242.72592554H220.72888889c-24.27259221 0-48.54518557-24.27259221-48.54518556-48.54518556s24.27259221-48.54518557 48.54518556-48.54518444h582.54222222c24.27259221 0 48.54518557 24.27259221 48.54518556 48.54518444s-24.27259221 48.54518557-48.54518556 48.54518556z" fill="currentColor"></path>
</svg>
      <span>CATEGORIES</span>
  </div>
  <div class="widget-body">
    <ul class="categories-list">
      
    </ul>
  </div>
</section>

      <section class="widget-tags widget-item  layout-margin content-padding--primary soft-size--large soft-style--box">
  <div class="widget-title">
    <svg class="icon icon-tags" viewBox="0 0 1098 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M283.42180005 272q0-28.38857157-20.09142843-48.48000001t-48.47999998-20.09142842-48.48000002 20.09142842-20.09142846 48.48000001 20.09142846 48.48 48.48000002 20.09142843 48.47999998-20.09142843 20.09142843-48.48zM855.0332285 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.03999997 263.58857157q-20.9142853 19.81714313-48.75428534 19.81714312-28.38857157 0-48.20571468-19.81714312l-383.04-383.58857157q-20.36571468-19.81714313-34.55999999-54.10285688t-14.19428534-62.6742853l0-222.85714313q0-27.84000002 20.36571469-48.20571469t48.2057147-20.36571466l222.85714313 0q28.38857157 0 62.6742853 14.19428529t54.65142842 34.55999999l383.04000001 382.49142843q19.81714313 20.9142853 19.81714314 48.75428532zM1060.74751475 580.57142843q0 28.38857157-19.81714313 48.2057147l-263.04 263.58857157q-20.9142853 19.81714313-48.75428531 19.81714312-19.26857155 0-31.61142843-7.47428531t-28.38857159-24.13714314l251.79428534-251.7942853q19.81714313-19.81714313 19.81714308-48.20571469 0-27.84000002-19.81714308-48.75428531l-383.04000001-382.49142845q-20.36571468-20.36571468-54.65142842-34.55999999t-62.67428532-14.19428534l120 0q28.38857157 0 62.67428532 14.19428534t54.65142842 34.55999999l383.03999998 382.49142845q19.81714313 20.9142853 19.81714314 48.75428531z" fill="currentColor"></path>
</svg>
    <span>TAGS</span>
  </div>
  <div class="widget-body">
    <div class="tags-cloud">
      
    </div>
  </div>
</section>
    </div>
  </article>
</div>

    <!-- footer container -->
<footer id="footer" class="footer">
  <div class="footer-container">
    
    <div class="social-icons">
      
        
      
        
      
        
      
        
          <a href="https://github.com/miiiku/" class="soft-size--primary soft-style--box" target="_blank" rel="noopener noreferrer">
            <svg class="icon icon-github" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M64.6 512c0 195.6 125.4 361.9 300.1 422.9 23.5 5.9 19.9-10.8 19.9-22.2v-77.6c-135.8 15.9-141.3-74-150.5-89-18.5-31.5-61.9-39.5-49-54.5 31-15.9 62.5 4 98.9 58 26.4 39.1 77.9 32.5 104.1 26 5.7-23.5 17.9-44.5 34.7-60.9-140.7-25.2-199.4-111.1-199.4-213.3 0-49.5 16.4-95.1 48.4-131.8-20.4-60.6 1.9-112.4 4.9-120.1 58.2-5.2 118.5 41.6 123.3 45.3 33.1-8.9 70.8-13.7 112.9-13.7 42.4 0 80.3 4.9 113.5 13.9 11.3-8.6 67.3-48.8 121.4-43.9 2.9 7.7 24.7 58.3 5.5 118.1 32.5 36.8 49 82.8 49 132.4 0 102.3-59 188.3-200.2 213.2 23.5 23.3 38.1 55.5 38.1 91.1v112.7c0.8 9 0 17.9 15.1 17.9C832.7 877 960.4 709.4 960.4 512.1c0-247.5-200.6-447.9-447.9-447.9C265 64.1 64.6 264.5 64.6 512z"></path>
</svg>
          </a>
        
      
        
          <a href="https://twitter.com/guanquanhong" class="soft-size--primary soft-style--box" target="_blank" rel="noopener noreferrer">
            <svg class="icon icon-twitter" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path d="M962.285714 233.142857q-38.285714 56-92.571429 95.428571 0.571429 8 0.571429 24 0 74.285714-21.714286 148.285714t-66 142-105.428571 120.285714-147.428571 83.428571-184.571429 31.142857q-154.857143 0-283.428571-82.857143 20 2.285714 44.571429 2.285714 128.571429 0 229.142857-78.857143-60-1.142857-107.428571-36.857143t-65.142857-91.142857q18.857143 2.857143 34.857143 2.857143 24.571429 0 48.571429-6.285714-64-13.142857-106-63.714286t-42-117.428571l0-2.285714q38.857143 21.714286 83.428571 23.428571-37.714286-25.142857-60-65.714286t-22.285714-88q0-50.285714 25.142857-93.142857 69.142857 85.142857 168.285714 136.285714t212.285714 56.857143q-4.571429-21.714286-4.571429-42.285714 0-76.571429 54-130.571429t130.571429-54q80 0 134.857143 58.285714 62.285714-12 117.142857-44.571429-21.142857 65.714286-81.142857 101.714286 53.142857-5.714286 106.285714-28.571429z"></path>
</svg>
          </a>
        
      
    </div>
     
    <p>&copy; 2025 <a href="/" target="_blank">寻玉</a></p>

    

    <p>Powered by <a href="https://hexo.io" target="_blank" rel="noopener noreferrer">Hexo</a> Theme - <a href="https://github.com/miiiku/flex-block" target="_blank" rel="noopener noreferrer author">flex-block</a></p>

    <p>
      <a href="javascript:;" id="theme-light">🌞 浅色</a>
      <a href="javascript:;" id="theme-dark">🌛 深色</a>
      <a href="javascript:;" id="theme-auto">🤖️ 自动</a>
    </p>
  </div>
</footer>
  </div>

  <div class="back-to-top-fixed soft-size--round soft-style--box">
    <svg class="icon icon-back-to-top" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
      <path d="M725.333333 426.666667c-12.8 0-21.333333-4.266667-29.866667-12.8l-213.333333-213.333333c-17.066667-17.066667-17.066667-42.666667 0-59.733333s42.666667-17.066667 59.733333 0l213.333333 213.333333c17.066667 17.066667 17.066667 42.666667 0 59.733333C746.666667 422.4 738.133333 426.666667 725.333333 426.666667z"></path>
      <path d="M298.666667 426.666667c-12.8 0-21.333333-4.266667-29.866667-12.8-17.066667-17.066667-17.066667-42.666667 0-59.733333l213.333333-213.333333c17.066667-17.066667 42.666667-17.066667 59.733333 0s17.066667 42.666667 0 59.733333l-213.333333 213.333333C320 422.4 311.466667 426.666667 298.666667 426.666667z"></path>
      <path d="M512 896c-25.6 0-42.666667-17.066667-42.666667-42.666667L469.333333 170.666667c0-25.6 17.066667-42.666667 42.666667-42.666667s42.666667 17.066667 42.666667 42.666667l0 682.666667C554.666667 878.933333 537.6 896 512 896z"></path>
    </svg>
  </div>

  
  






<!-- copy button  -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script>

<!-- https://clipboardjs.com/ -->










  


  


  




<script src="/js/script.js"></script>


  
  <!-- 尾部用户自定义相关内容 -->
</body>
</html>
